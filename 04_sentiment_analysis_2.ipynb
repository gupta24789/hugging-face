{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/hugging-face/blob/main/04_sentiment_analysis_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TSx6W8DUvXh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGs7KOWEUvXi"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset, Features, ClassLabel, Value\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AxLxegsUvXj"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnXR1MMHUvXl",
        "outputId": "cefbd146-db48-4959-aea2-3236210cfdce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8004\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"sg247/binary-classification\", data_files= {\"train\": \"train.csv\", \"test\":\"test.csv\"})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2NAT-60UvXm",
        "outputId": "65e54e71-b534-446f-d09c-c264402f4dcd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': Value(dtype='string', id=None),\n",
              " 'label': Value(dtype='float64', id=None)}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Label must be of type ClassLabel\n",
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrm7oVpWUvXn",
        "outputId": "d507eaa1-da05-47f5-c86c-6921f4783abb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8004\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## label must of ClassLabel Type\n",
        "features = Features({\"tweet\": Value(dtype = \"string\"), \"label\": ClassLabel(num_classes=2, names=[0,1])})\n",
        "dataset = load_dataset(\"sg247/binary-classification\", data_files= {\"train\": \"train.csv\", \"test\":\"test.csv\"}, features = features)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gYoOUKBUvXo",
        "outputId": "4170cf57-c3d1-4b74-9d5f-a6316f2e2631"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=[0, 1], id=None)}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aG4CrBXUvXo"
      },
      "source": [
        "## Remove NA from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbTxihjmUvXp",
        "outputId": "4c82b9d3-8f47-4a36-b1a7-d869bd180443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.filter(lambda x: x['tweet'] is not None and x['label'] is not None and len(x['tweet'])>0)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFhhe06WUvXp",
        "outputId": "105edfca-984e-4b9e-cba8-18acd739ae20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': 'Want to say a huge thanks to @WarriorAssaultS @uktac @BolleSafety @Mechanix_Wear @Airtech_Studios @Hexmags #FF Thanks for the support :)',\n",
              " 'label': 1}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTa8WYK3UvXq"
      },
      "source": [
        "## Tokenized Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jc2sEv6UvXq",
        "outputId": "52de5f23-fb85-448c-d3d0-9e7766d897fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_tweet(row):\n",
        "    return tokenizer(row['tweet'], padding='max_length', truncation=True, max_length=50)\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_datasets = dataset.map(tokenize_tweet)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5Wy-RKLUvXr",
        "outputId": "a2fda120-1e3d-4d1c-cdb9-cf2bf05c9379"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## remove tweet column and rename label as labels\n",
        "tokenized_datasets = tokenized_datasets.remove_columns('tweet')\n",
        "tokenized_datasets = tokenized_datasets.rename_columns({\"label\":\"labels\"})\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb4bGtJEUvXs"
      },
      "outputs": [],
      "source": [
        "## create train and test dataset\n",
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
        "eval_dataset = tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkfXtnuWUvXs",
        "outputId": "01e5a64a-73fc-43af-aa53-66d032804aa5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 8000\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKLuhNoQUvXt"
      },
      "source": [
        "## Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRixITjnUvXt"
      },
      "outputs": [],
      "source": [
        "def custom_collator(batch):\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    token_type_ids = torch.tensor([item['token_type_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "\n",
        "    return {\"labels\": labels, \"input_ids\":input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU53mQN4UvXu"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXedDV32UvXu",
        "outputId": "d3f00672-3649-443c-f648-cf6e89f373df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torchmetrics.classification.precision_recall.Recall"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchmetrics.Precision\n",
        "torchmetrics.Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzWP5X-qUvXv"
      },
      "outputs": [],
      "source": [
        "class SentimentModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, output_dim, learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        ## layers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.transformer = AutoModel.from_pretrained(model_name)\n",
        "        self.linear = nn.Linear(self.transformer.config.hidden_size, output_dim)\n",
        "\n",
        "        ## loss\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "        ## metrics\n",
        "        self.accuracy = torchmetrics.Accuracy(task = 'binary', num_classes = output_dim)\n",
        "        self.f1_score = torchmetrics.F1Score(task = 'binary', num_classes = output_dim)\n",
        "        self.precision = torchmetrics.Precision(task = 'binary', num_classes = output_dim)\n",
        "        self.recall = torchmetrics.Recall(task = 'binary', num_classes = output_dim)\n",
        "\n",
        "\n",
        "    def compute_metrics(self, logits, references):\n",
        "        logits = torch.argmax(logits, axis = 1)\n",
        "        self.accuracy(logits, references)\n",
        "        self.f1_score(logits, references)\n",
        "        self.precision(logits, references)\n",
        "        self.recall(logits, references)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.transformer(**inputs)\n",
        "        # output has two keys : last_hidden_state, pooler_output\n",
        "        logits = self.linear(output['pooler_output'])\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        labels, inputs = batch.pop('labels'), batch\n",
        "        logits = self(inputs)\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        self.train_loss.append(loss.item())\n",
        "        self.log_dict({\"train_loss\": loss}, on_step = False, on_epoch = True, prog_bar = True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        labels, inputs = batch.pop('labels'), batch\n",
        "        logits = self(inputs)\n",
        "        loss = self.loss_fn(logits, labels)\n",
        "        self.val_loss.append(loss.item())\n",
        "        self.log_dict({\"val_loss\": loss}, on_step = False, on_epoch = True, prog_bar = True)\n",
        "        self.compute_metrics(logits, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        metrics = {\n",
        "            \"Epoch\": self.current_epoch,\n",
        "            \"Train Loss\" : np.mean(self.train_loss),\n",
        "            \"Val Loss\": np.mean(self.val_loss),\n",
        "            \"Accuracy \": self.accuracy.compute().item(),\n",
        "            \"F1\": self.f1_score.compute().item(),\n",
        "            \"Precision\": self.precision.compute().item(),\n",
        "            \"Recall\": self.recall.compute().item()\n",
        "        }\n",
        "\n",
        "        print(pd.DataFrame(metrics.items()).T)\n",
        "\n",
        "        self.train_loss =[]\n",
        "        self.val_loss = []\n",
        "        self.accuracy.reset()\n",
        "        self.f1_score.reset()\n",
        "        self.precision.reset()\n",
        "        self.recall.reset()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return optim.Adam(self.parameters(), lr = self.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDvEnlOuUvXw"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GQhRsh0UvXw"
      },
      "outputs": [],
      "source": [
        "## config\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_EPOCHS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77LBSs0YUvXw"
      },
      "outputs": [],
      "source": [
        "train_dl = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True, collate_fn = custom_collator, num_workers = 1)\n",
        "test_dl = DataLoader(eval_dataset, batch_size = BATCH_SIZE, shuffle = False, collate_fn = custom_collator, num_workers = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeCpSHjCUvXw"
      },
      "outputs": [],
      "source": [
        "model = SentimentModel(output_dim=2, learning_rate= LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw9UlbkxUvXx",
        "outputId": "760440cd-d348-4d9e-9d36-247037192d39"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        dirpath = \"checkpoints_logs\",\n",
        "        filename = '{epoch}-{val_loss:.2f}-{val_accuracy:.2f}',\n",
        "        monitor = \"val_loss\",\n",
        "        save_last = True,\n",
        "        save_top_k = -1\n",
        ")\n",
        "\n",
        "early_stoping_callback = pl.callbacks.EarlyStopping(\n",
        "            monitor = \"val_loss\",\n",
        "            min_delta = 0.001,\n",
        "            patience = 3,\n",
        "            mode = \"min\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "        accelerator = \"gpu\",\n",
        "        callbacks = [checkpoint_callback, early_stoping_callback],\n",
        "        max_epochs = NUM_EPOCHS,\n",
        "        check_val_every_n_epoch = 1,\n",
        "        gradient_clip_val = 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBjXU9egUvXx",
        "outputId": "4755f1f7-16f3-4f6b-b1e6-1e2ea18956e7",
        "colab": {
          "referenced_widgets": [
            "c4f0480c544c48f68fa18308c545ceef",
            "1d511c40cdea4c72b8338cf3b7a0e700",
            "157b7419dadf49df839c10f1eafa8dda",
            "0420cde5a8814b79bb1e31438c977666"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type             | Params\n",
            "-------------------------------------------------\n",
            "0 | transformer | BertModel        | 109 M \n",
            "1 | linear      | Linear           | 1.5 K \n",
            "2 | loss_fn     | CrossEntropyLoss | 0     \n",
            "3 | accuracy    | BinaryAccuracy   | 0     \n",
            "4 | f1_score    | BinaryF1Score    | 0     \n",
            "5 | precision   | BinaryPrecision  | 0     \n",
            "6 | recall      | BinaryRecall     | 0     \n",
            "-------------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "437.935   Total estimated model params size (MB)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4f0480c544c48f68fa18308c545ceef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       0           1         2          3         4          5       6\n",
            "0  Epoch  Train Loss  Val Loss  Accuracy         F1  Precision  Recall\n",
            "1    0.0         NaN   0.76561     0.1875  0.315789        1.0  0.1875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/home/saurabh/anaconda3/envs/lighting/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d511c40cdea4c72b8338cf3b7a0e700",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "157b7419dadf49df839c10f1eafa8dda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       0           1         2          3         4          5       6\n",
            "0  Epoch  Train Loss  Val Loss  Accuracy         F1  Precision  Recall\n",
            "1    0.0    0.071429  0.013807      0.997  0.997003   0.996008   0.998\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0420cde5a8814b79bb1e31438c977666",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Validation: |          | 0/? [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       0           1         2          3         4          5       6\n",
            "0  Epoch  Train Loss  Val Loss  Accuracy         F1  Precision  Recall\n",
            "1    1.0    0.005805  0.011801      0.997  0.997003   0.996008   0.998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
          ]
        }
      ],
      "source": [
        "trainer.fit(model, train_dl, test_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNk33CDvUvXx"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-40wNOwUvXx"
      },
      "outputs": [],
      "source": [
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1dQvNnIUvXy",
        "outputId": "0d114aba-a7dc-49f7-9b43-d75807fc7dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet : thinking about @StereoKicks again :(\n",
            "True : 0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pred : 0\n"
          ]
        }
      ],
      "source": [
        "sample = dataset['test'].shuffle()[0]\n",
        "tweet, label = sample['tweet'], sample['label']\n",
        "\n",
        "print(f'Tweet : {tweet}')\n",
        "print(f'True : {label}')\n",
        "\n",
        "inputs =tokenizer(tweet, return_tensors='pt')\n",
        "logits = model(inputs).detach()\n",
        "preds = torch.argmax(logits, axis=1)[0].item()\n",
        "print(f'Pred : {preds}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}