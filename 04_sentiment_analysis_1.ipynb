{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gupta24789/hugging-face/blob/main/04_sentiment_analysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut1CvzWc8wFk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csG-MFPf8wFn"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset, Features, ClassLabel, Value\n",
        "from transformers import AutoTokenizer, TrainingArguments,Trainer, AutoModelForSequenceClassification\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWJFq2iS8wFp"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRd2NU-b8wFr",
        "outputId": "fd0f7e1c-70a5-4f27-87d6-ee2fcfdd22a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8004\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"sg247/binary-classification\", data_files= {\"train\": \"train.csv\", \"test\":\"test.csv\"})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PqAZHdB8wFs",
        "outputId": "a2aa306d-aee3-438a-bec0-d44deecc22a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': Value(dtype='string', id=None),\n",
              " 'label': Value(dtype='float64', id=None)}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Label must be of type ClassLabel\n",
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGqe5BBg8wFt",
        "outputId": "2efb10ad-71a3-4c80-82c0-ee845a10f0d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8004\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## label must of ClassLabel Type\n",
        "features = Features({\"tweet\": Value(dtype = \"string\"), \"label\": ClassLabel(num_classes=2, names=[0,1])})\n",
        "dataset = load_dataset(\"sg247/binary-classification\", data_files= {\"train\": \"train.csv\", \"test\":\"test.csv\"}, features = features)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NvD5dEE8wFu",
        "outputId": "1b3a3268-aad0-4c72-b737-b109da579a49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=[0, 1], id=None)}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'].features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmy7XoRL8wFv"
      },
      "source": [
        "## Remove NA from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6wmP8AV8wFv",
        "outputId": "398aebb8-2445-440b-d3af-b6a38a83fec5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = dataset.filter(lambda x: x['tweet'] is not None and x['label'] is not None and len(x['tweet'])>0)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cpPUcsb8wFw",
        "outputId": "da46564b-365a-41e0-dc68-f389704fa4a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'tweet': 'Want to say a huge thanks to @WarriorAssaultS @uktac @BolleSafety @Mechanix_Wear @Airtech_Studios @Hexmags #FF Thanks for the support :)',\n",
              " 'label': 1}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLOhNXm8wFw"
      },
      "source": [
        "## Tokenized Tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftrSRP7p8wFx",
        "outputId": "cf2b9c54-ad09-44c1-b05d-83e801a278d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['tweet', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize_tweet(row):\n",
        "    return tokenizer(row['tweet'], padding='max_length', truncation=True, max_length=50)\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_datasets = dataset.map(tokenize_tweet)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBb-kpnc8wFy",
        "outputId": "44683e9b-bc74-4d0b-d9e5-8072b0362e25"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 8000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## remove tweet column and rename label as labels\n",
        "tokenized_datasets = tokenized_datasets.remove_columns('tweet')\n",
        "tokenized_datasets = tokenized_datasets.rename_columns({\"label\":\"labels\"})\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KyG-sDk8wFy"
      },
      "outputs": [],
      "source": [
        "## create train and test dataset\n",
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
        "eval_dataset = tokenized_datasets[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpZNGv4O8wFy"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v53ZgfAK8wFz"
      },
      "outputs": [],
      "source": [
        "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0oXjNFy8wFz"
      },
      "source": [
        "## Trainer Arguments\n",
        "\n",
        "##### evaluation_strategy\n",
        "    - \"no\": No evaluation is done during training.\n",
        "    - \"steps\": Evaluation is done (and logged) every eval_steps.\n",
        "    - \"epoch\": Evaluation is done at the end of each epoch.\n",
        "\n",
        "##### logging_dir (str, optional)\n",
        "    — TensorBoard log directory. Will default to *output_dir/runs/CURRENT_DATETIME_HOSTNAME*.\n",
        "\n",
        "##### logging_strategy (str or IntervalStrategy, optional, defaults to \"steps\")\n",
        "    — The logging strategy to adopt during training. Possible values are:\n",
        "        - \"no\": No logging is done during training.\n",
        "        - \"epoch\": Logging is done at the end of each epoch.\n",
        "        - \"steps\": Logging is done every logging_steps.\n",
        "\n",
        "##### run_name (str, optional) — A descriptor for the run. Typically used for wandb and mlflow logging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zp9z74IT8wF0",
        "outputId": "576766c8-ed96-4cad-bc1c-8ea09c615a20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [126/126 01:26, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.609800</td>\n",
              "      <td>0.255135</td>\n",
              "      <td>0.994500</td>\n",
              "      <td>0.994508</td>\n",
              "      <td>0.993021</td>\n",
              "      <td>0.996000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.193900</td>\n",
              "      <td>0.019086</td>\n",
              "      <td>0.997000</td>\n",
              "      <td>0.997003</td>\n",
              "      <td>0.996008</td>\n",
              "      <td>0.998000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=126, training_loss=0.32458121861730305, metrics={'train_runtime': 88.1827, 'train_samples_per_second': 181.442, 'train_steps_per_second': 1.429, 'total_flos': 411111024000000.0, 'train_loss': 0.32458121861730305, 'epoch': 2.0})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2, id2label = id2label, label2id = label2id)\n",
        "\n",
        "\n",
        "## CONFIG\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "## Epoch Level Logging\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"checkpoints_logs\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    learning_rate = LEARNING_RATE,\n",
        "    num_train_epochs = NUM_EPOCHS,\n",
        "    warmup_steps=100,\n",
        "    weight_decay= 0.01,\n",
        "    logging_dir = \"logs\",\n",
        "    logging_steps = 50,\n",
        "    use_cpu  = False,\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "## Trainer config\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "## Train Model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jirGcboe8wF1"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Cki7dAB8wF1"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"checkpoints_logs/checkpoint-126/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEAhtfek8wF1"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp1ZjG428wF1"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline('sentiment-analysis', model = model, tokenizer= tokenizer, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJf4WxKK8wF2",
        "outputId": "416eb919-33ab-49d3-92bf-7d3b192e8780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tweet : @Youmeatyours yeah its horrible isn't it :( big hugs! &amp; it means a lot. X\n",
            "True : NEGATIVE\n",
            "Pred : NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "data = dataset['test'].shuffle()[0]\n",
        "tweet, label = data['tweet'], data['label']\n",
        "print(f\"Tweet : {tweet}\")\n",
        "print(f\"True : {'POSITIVE' if 1==label else 'NEGATIVE'}\")\n",
        "print(f\"Pred : {classifier(tweet)[0]['label']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMVXlPS58wF2"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9U9s5Wiz8wF2"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig9RrrJa8wF2"
      },
      "outputs": [],
      "source": [
        "# from tensorboard import notebook\n",
        "# notebook.list()\n",
        "# notebook.display(port=6006, height=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrOTIz0t8wF3"
      },
      "outputs": [],
      "source": [
        "## start in terminal\n",
        "## tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Z7TyHM8wF3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lighting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}